{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d101809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net from scratch with line-by-line comments explaining literally everything.\n",
    "\n",
    "import torch                         # Core PyTorch tensor library\n",
    "import torch.nn as nn                # Neural network layers and modules\n",
    "import torch.nn.functional as F      # Functional ops (conv, relu, interpolate, etc.)\n",
    "\n",
    "# ---------------------------\n",
    "# Basic building block: DoubleConv\n",
    "# ---------------------------\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Two consecutive 3x3 convolutions with ReLU activations (no change in H/W thanks to padding=1).\n",
    "    This is the standard U-Net \"conv -> ReLU -> conv -> ReLU\" pattern used everywhere.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super().__init__()                                  # Initialize nn.Module internals\n",
    "        self.net = nn.Sequential(                           # Sequential container runs layers in order\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=True),  # 1st 3x3 conv (keeps H/W)\n",
    "            nn.ReLU(inplace=True),                          # Nonlinearity (inplace saves a bit of memory)\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=True), # 2nd 3x3 conv\n",
    "            nn.ReLU(inplace=True),                          # Nonlinearity\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: shape [N, C_in, H, W]\n",
    "        return self.net(x)                                  # Output: [N, C_out, H, W]\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Down block: DoubleConv then downsample (MaxPool)\n",
    "# ---------------------------\n",
    "class Down(nn.Module):\n",
    "    \"\"\"\n",
    "    Contracting-path block: extracts features (DoubleConv) then halves spatial size (MaxPool).\n",
    "    Returns both the pooled output (for next stage) and the pre-pooled features (skip connection).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super().__init__()\n",
    "        self.encode = DoubleConv(in_ch, out_ch)            # Feature extractor at this scale\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # Halve H and W\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # x: [N, C_in, H, W]\n",
    "        feat = self.encode(x)                              # feat: [N, out_ch, H, W] (skip tensor)\n",
    "        x_down = self.pool(feat)                           # x_down: [N, out_ch, H/2, W/2]\n",
    "        return x_down, feat                                # Return both for later use\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Up block: upsample (by transpose conv or bilinear+1x1) then DoubleConv after concatenating skip\n",
    "# ---------------------------\n",
    "class Up(nn.Module):\n",
    "    \"\"\"\n",
    "    Expansive-path block: upsample decoder features to match the encoder scale,\n",
    "    concatenate the corresponding skip features, then fuse with DoubleConv.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch: int, skip_ch: int, out_ch: int, up_mode: str = \"transpose\"):\n",
    "        \"\"\"\n",
    "        in_ch:   channels of incoming decoder feature (before upsampling)\n",
    "        skip_ch: channels of the encoder skip tensor to concatenate\n",
    "        out_ch:  channels after fusion\n",
    "        up_mode: \"transpose\" (learned ConvTranspose2d) or \"bilinear\" (Upsample + 1x1 Conv)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.up_mode = up_mode                             # Store chosen upsampling method\n",
    "\n",
    "        if up_mode == \"transpose\":\n",
    "            # Learnable upsampling that doubles H/W when stride=2\n",
    "            self.up = nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2)\n",
    "            # After upsampling, decoder tensor has out_ch channels; after concat with skip (skip_ch),\n",
    "            # the DoubleConv in_channels = out_ch + skip_ch (defined below).\n",
    "            reduce_out = out_ch                            # No extra reduce layer needed\n",
    "            self.reduce = None\n",
    "        elif up_mode == \"bilinear\":\n",
    "            # Fixed (non-learned) resizing to double H/W\n",
    "            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "            # 1x1 conv to adjust channel count after upsampling\n",
    "            self.reduce = nn.Conv2d(in_ch, out_ch, kernel_size=1)\n",
    "            reduce_out = out_ch\n",
    "        else:\n",
    "            raise ValueError(\"up_mode must be 'transpose' or 'bilinear'.\")\n",
    "\n",
    "        # After upsampling and (optional) reduce, we concatenate along channel dim with skip features.\n",
    "        # Therefore, the DoubleConv takes (reduce_out + skip_ch) channels in, outputs out_ch channels.\n",
    "        self.fuse = DoubleConv(reduce_out + skip_ch, out_ch)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, skip: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x:    decoder tensor to be upsampled, shape [N, C_in, H_dec, W_dec]\n",
    "        skip: encoder skip tensor, shape [N, C_skip, H_skip, W_skip]\n",
    "        returns fused feature at skip's resolution: [N, out_ch, H_skip, W_skip]\n",
    "        \"\"\"\n",
    "        if self.up_mode == \"transpose\":\n",
    "            x = self.up(x)                                 # Learned upsample: doubles H and W\n",
    "        else:\n",
    "            x = self.up(x)                                 # Bilinear resize (no channel change)\n",
    "            x = self.reduce(x)                             # 1x1 conv to set channels to out_ch\n",
    "\n",
    "        # At this point, due to odd sizes or padding choices, shapes may not match perfectly.\n",
    "        # We center-pad the upsampled x to match skip's H and W so concatenation is legal.\n",
    "        diff_h = skip.size(2) - x.size(2)                  # Height difference\n",
    "        diff_w = skip.size(3) - x.size(3)                  # Width difference\n",
    "\n",
    "        # F.pad pad order for 2D: (pad_left, pad_right, pad_top, pad_bottom)\n",
    "        # We split the difference so x is centered relative to skip.\n",
    "        x = F.pad(x, [diff_w // 2, diff_w - diff_w // 2,\n",
    "                      diff_h // 2, diff_h - diff_h // 2])\n",
    "\n",
    "        # Concatenate along channels: decoder upsampled feature + encoder skip feature\n",
    "        x = torch.cat([skip, x], dim=1)                    # Shape: [N, C_skip + C_dec_up, H_skip, W_skip]\n",
    "\n",
    "        # Fuse the concatenated features with two 3x3 convs (DoubleConv)\n",
    "        x = self.fuse(x)                                   # Shape: [N, out_ch, H_skip, W_skip]\n",
    "        return x\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Full U-Net\n",
    "# ---------------------------\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Classic U-Net with 4 downsampling steps and 4 upsampling steps.\n",
    "    - Encoder (Down blocks): extract context, shrink spatial size.\n",
    "    - Bottleneck (DoubleConv): deepest features.\n",
    "    - Decoder (Up blocks): restore spatial size, fuse with encoder skips for fine detail.\n",
    "    - Final 1x1 conv: map to desired number of classes/channels.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch: int = 1, out_ch: int = 1, up_mode: str = \"transpose\"):\n",
    "        \"\"\"\n",
    "        in_ch:  number of input channels (1 for grayscale, 3 for RGB)\n",
    "        out_ch: number of output channels (1 for binary mask logits, K for K-class logits)\n",
    "        up_mode: \"transpose\" or \"bilinear\" upsampling method\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Feature sizes in the encoder; double as we go down to increase capacity.\n",
    "        f1, f2, f3, f4 = 64, 128, 256, 512\n",
    "\n",
    "        # Initial high-resolution feature extraction (no pooling here).\n",
    "        self.in_conv = DoubleConv(in_ch, f1)               # [N, f1, H, W]\n",
    "\n",
    "        # Contracting path (each Down returns next tensor and skip features)\n",
    "        self.down1 = Down(f1, f2)                          # -> [N, f2, H/2, W/2], skip f2@H,W\n",
    "        self.down2 = Down(f2, f3)                          # -> [N, f3, H/4, W/4], skip f3@H/2,W/2\n",
    "        self.down3 = Down(f3, f4)                          # -> [N, f4, H/8, W/8], skip f4@H/4,W/4\n",
    "\n",
    "        # One more downsampling to the bottleneck scale (like original UNet's 4 pools total)\n",
    "        self.pool = nn.MaxPool2d(2)                        # Simple pooling layer to reach bottleneck\n",
    "        self.bottleneck = DoubleConv(f4, f4 * 2)           # Deepest features: channels doubled (1024)\n",
    "\n",
    "        # Expansive path: upsample and fuse with corresponding skips\n",
    "        # Up(in_ch from decoder, skip_ch from encoder, out_ch after fusion)\n",
    "        self.up3 = Up(in_ch=f4 * 2, skip_ch=f4, out_ch=f4, up_mode=up_mode)  # match down3 skip\n",
    "        self.up2 = Up(in_ch=f4,     skip_ch=f3, out_ch=f3, up_mode=up_mode)  # match down2 skip\n",
    "        self.up1 = Up(in_ch=f3,     skip_ch=f2, out_ch=f2, up_mode=up_mode)  # match down1 skip\n",
    "        self.up0 = Up(in_ch=f2,     skip_ch=f1, out_ch=f1, up_mode=up_mode)  # match in_conv output\n",
    "\n",
    "        # Final 1x1 conv maps features to output channels (e.g., 1 for binary, K for classes)\n",
    "        self.out_conv = nn.Conv2d(f1, out_ch, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: input image tensor [N, in_ch, H, W]\n",
    "        returns: logits tensor [N, out_ch, H, W]\n",
    "        \"\"\"\n",
    "        # Encoder\n",
    "        x0 = self.in_conv(x)                               # Highest-res features, skip0\n",
    "        x1, skip1 = self.down1(x0)                         # Down once, keep skip1 (features before pool)\n",
    "        x2, skip2 = self.down2(x1)                         # Down twice, keep skip2\n",
    "        x3, skip3 = self.down3(x2)                         # Down thrice, keep skip3\n",
    "\n",
    "        # Bottleneck\n",
    "        x4 = self.pool(x3)                                 # Final downsample to bottleneck scale\n",
    "        x4 = self.bottleneck(x4)                           # Deep features (context)\n",
    "\n",
    "        # Decoder (upsample + skip concat + fuse)\n",
    "        x = self.up3(x4, skip3)                            # Up to match skip3 spatial size\n",
    "        x = self.up2(x,  skip2)                            # Up to match skip2 spatial size\n",
    "        x = self.up1(x,  skip1)                            # Up to match skip1 spatial size\n",
    "        x = self.up0(x,  x0)                               # Up to match input spatial size\n",
    "\n",
    "        # Project to desired output channels (logits, not probabilities)\n",
    "        logits = self.out_conv(x)                          # [N, out_ch, H, W]\n",
    "        return logits\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
